{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/khushpatel/Desktop/ML Projects/Emotion Detection/audio_features.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['YAF_disgust', 'OAF_Pleasant_surprise', 'OAF_happy', 'YAF_sad',\n",
       "       'YAF_happy', 'YAF_neutral', 'OAF_Fear', 'OAF_angry',\n",
       "       'YAF_pleasant_surprised', 'YAF_fear', 'OAF_neutral', 'OAF_disgust',\n",
       "       'YAF_angry', 'OAF_Sad'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disgust' 'pleasant' 'happy' 'sad' 'neutral' 'fear' 'angry']\n"
     ]
    }
   ],
   "source": [
    "#Merging emotions of YAF and OAF \n",
    "\n",
    "def normalized_labels(label):\n",
    "    return label.split('_')[1].lower()\n",
    "\n",
    "df['label'] = df['label'].apply(normalized_labels)\n",
    "print(df['label'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      label_angry  label_disgust  label_fear  label_happy  label_neutral  \\\n",
      "0             0.0            1.0         0.0          0.0            0.0   \n",
      "1             0.0            1.0         0.0          0.0            0.0   \n",
      "2             0.0            1.0         0.0          0.0            0.0   \n",
      "3             0.0            1.0         0.0          0.0            0.0   \n",
      "4             0.0            1.0         0.0          0.0            0.0   \n",
      "...           ...            ...         ...          ...            ...   \n",
      "2795          0.0            0.0         0.0          0.0            0.0   \n",
      "2796          0.0            0.0         0.0          0.0            0.0   \n",
      "2797          0.0            0.0         0.0          0.0            0.0   \n",
      "2798          0.0            0.0         0.0          0.0            0.0   \n",
      "2799          0.0            0.0         0.0          0.0            0.0   \n",
      "\n",
      "      label_pleasant  label_sad  \n",
      "0                0.0        0.0  \n",
      "1                0.0        0.0  \n",
      "2                0.0        0.0  \n",
      "3                0.0        0.0  \n",
      "4                0.0        0.0  \n",
      "...              ...        ...  \n",
      "2795             0.0        1.0  \n",
      "2796             0.0        1.0  \n",
      "2797             0.0        1.0  \n",
      "2798             0.0        1.0  \n",
      "2799             0.0        1.0  \n",
      "\n",
      "[2800 rows x 7 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
       "       ...\n",
       "       '163', '164', '165', 'label_angry', 'label_disgust', 'label_fear',\n",
       "       'label_happy', 'label_neutral', 'label_pleasant', 'label_sad'],\n",
       "      dtype='object', length=173)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#OneHot encoding \n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit(df[['label']])\n",
    "encoded_labels = pd.DataFrame(encoder.transform(df[['label']]).toarray(), columns = encoder.get_feature_names_out(['label']))\n",
    "print(encoded_labels)\n",
    "df_encoded = pd.concat([df, encoded_labels], axis=1)\n",
    "\n",
    "# Drop the original 'label' column\n",
    "df_encoded.drop(columns=['label'], inplace=True)\n",
    "\n",
    "df_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['label_angry', 'label_disgust', 'label_fear',\n",
    "       'label_happy', 'label_neutral', 'label_pleasant', 'label_sad']\n",
    "\n",
    "X = df_encoded.drop(columns=target)\n",
    "Y = df_encoded[target]\n",
    "\n",
    "scaled = StandardScaler()\n",
    "X_scaled = scaled.fi\n",
    "\n",
    "X_train,X_test,y_train,y_test =  train_test_split(X,Y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2016/2016 [==============================] - 2s 648us/step - loss: 5.9183 - accuracy: 0.1954 - val_loss: 2.1771 - val_accuracy: 0.3304\n",
      "Epoch 2/100\n",
      "2016/2016 [==============================] - 1s 557us/step - loss: 1.8855 - accuracy: 0.3611 - val_loss: 1.8229 - val_accuracy: 0.4420\n",
      "Epoch 3/100\n",
      "2016/2016 [==============================] - 1s 613us/step - loss: 1.5208 - accuracy: 0.5293 - val_loss: 1.4009 - val_accuracy: 0.6071\n",
      "Epoch 4/100\n",
      "2016/2016 [==============================] - 1s 566us/step - loss: 1.2433 - accuracy: 0.6587 - val_loss: 1.1527 - val_accuracy: 0.7054\n",
      "Epoch 5/100\n",
      "2016/2016 [==============================] - 1s 566us/step - loss: 1.0710 - accuracy: 0.7178 - val_loss: 1.0511 - val_accuracy: 0.7411\n",
      "Epoch 6/100\n",
      "2016/2016 [==============================] - 1s 557us/step - loss: 0.9405 - accuracy: 0.7738 - val_loss: 0.9228 - val_accuracy: 0.7589\n",
      "Epoch 7/100\n",
      "2016/2016 [==============================] - 1s 556us/step - loss: 0.8499 - accuracy: 0.8070 - val_loss: 0.8557 - val_accuracy: 0.7946\n",
      "Epoch 8/100\n",
      "2016/2016 [==============================] - 1s 568us/step - loss: 0.7810 - accuracy: 0.8239 - val_loss: 0.7500 - val_accuracy: 0.8348\n",
      "Epoch 9/100\n",
      "2016/2016 [==============================] - 1s 557us/step - loss: 0.7144 - accuracy: 0.8487 - val_loss: 0.6888 - val_accuracy: 0.8750\n",
      "Epoch 10/100\n",
      "2016/2016 [==============================] - 1s 557us/step - loss: 0.6684 - accuracy: 0.8681 - val_loss: 0.6659 - val_accuracy: 0.8750\n",
      "Epoch 11/100\n",
      "2016/2016 [==============================] - 1s 560us/step - loss: 0.6237 - accuracy: 0.8790 - val_loss: 0.6801 - val_accuracy: 0.8214\n",
      "Epoch 12/100\n",
      "2016/2016 [==============================] - 1s 565us/step - loss: 0.5949 - accuracy: 0.8899 - val_loss: 0.6293 - val_accuracy: 0.8705\n",
      "Epoch 13/100\n",
      "2016/2016 [==============================] - 1s 586us/step - loss: 0.5713 - accuracy: 0.8919 - val_loss: 0.5636 - val_accuracy: 0.8929\n",
      "Epoch 14/100\n",
      "2016/2016 [==============================] - 1s 578us/step - loss: 0.5500 - accuracy: 0.8993 - val_loss: 0.5568 - val_accuracy: 0.8973\n",
      "Epoch 15/100\n",
      "2016/2016 [==============================] - 1s 583us/step - loss: 0.5291 - accuracy: 0.9127 - val_loss: 0.5591 - val_accuracy: 0.8973\n",
      "Epoch 16/100\n",
      "2016/2016 [==============================] - 1s 598us/step - loss: 0.5096 - accuracy: 0.9216 - val_loss: 0.5290 - val_accuracy: 0.9152\n",
      "Epoch 17/100\n",
      "2016/2016 [==============================] - 1s 558us/step - loss: 0.4991 - accuracy: 0.9112 - val_loss: 0.5418 - val_accuracy: 0.8973\n",
      "Epoch 18/100\n",
      "2016/2016 [==============================] - 1s 573us/step - loss: 0.4837 - accuracy: 0.9226 - val_loss: 0.5387 - val_accuracy: 0.8839\n",
      "Epoch 19/100\n",
      "2016/2016 [==============================] - 2s 961us/step - loss: 0.4706 - accuracy: 0.9231 - val_loss: 0.5097 - val_accuracy: 0.9152\n",
      "Epoch 20/100\n",
      "2016/2016 [==============================] - 1s 572us/step - loss: 0.4594 - accuracy: 0.9320 - val_loss: 0.5058 - val_accuracy: 0.9062\n",
      "Epoch 21/100\n",
      "2016/2016 [==============================] - 2s 888us/step - loss: 0.4503 - accuracy: 0.9315 - val_loss: 0.4908 - val_accuracy: 0.9062\n",
      "Epoch 22/100\n",
      "2016/2016 [==============================] - 1s 549us/step - loss: 0.4396 - accuracy: 0.9395 - val_loss: 0.4656 - val_accuracy: 0.9286\n",
      "Epoch 23/100\n",
      "2016/2016 [==============================] - 1s 559us/step - loss: 0.4344 - accuracy: 0.9335 - val_loss: 0.4800 - val_accuracy: 0.9062\n",
      "Epoch 24/100\n",
      "2016/2016 [==============================] - 1s 543us/step - loss: 0.4289 - accuracy: 0.9345 - val_loss: 0.4687 - val_accuracy: 0.9196\n",
      "Epoch 25/100\n",
      "2016/2016 [==============================] - 1s 571us/step - loss: 0.4193 - accuracy: 0.9400 - val_loss: 0.4793 - val_accuracy: 0.9286\n",
      "Epoch 26/100\n",
      "2016/2016 [==============================] - 1s 540us/step - loss: 0.4159 - accuracy: 0.9430 - val_loss: 0.4351 - val_accuracy: 0.9286\n",
      "Epoch 27/100\n",
      "2016/2016 [==============================] - 1s 595us/step - loss: 0.4080 - accuracy: 0.9435 - val_loss: 0.6589 - val_accuracy: 0.8214\n",
      "Epoch 28/100\n",
      "2016/2016 [==============================] - 1s 622us/step - loss: 0.4020 - accuracy: 0.9415 - val_loss: 0.4525 - val_accuracy: 0.9286\n",
      "Epoch 29/100\n",
      "2016/2016 [==============================] - 1s 605us/step - loss: 0.3927 - accuracy: 0.9529 - val_loss: 0.4252 - val_accuracy: 0.9375\n",
      "Epoch 30/100\n",
      "2016/2016 [==============================] - 1s 544us/step - loss: 0.3919 - accuracy: 0.9435 - val_loss: 0.4318 - val_accuracy: 0.9330\n",
      "Epoch 31/100\n",
      "2016/2016 [==============================] - 1s 562us/step - loss: 0.3885 - accuracy: 0.9514 - val_loss: 0.4254 - val_accuracy: 0.9375\n",
      "Epoch 32/100\n",
      "2016/2016 [==============================] - 1s 574us/step - loss: 0.3751 - accuracy: 0.9559 - val_loss: 0.4051 - val_accuracy: 0.9509\n",
      "Epoch 33/100\n",
      "2016/2016 [==============================] - 1s 556us/step - loss: 0.3773 - accuracy: 0.9499 - val_loss: 0.4418 - val_accuracy: 0.9330\n",
      "Epoch 34/100\n",
      "2016/2016 [==============================] - 1s 541us/step - loss: 0.3768 - accuracy: 0.9459 - val_loss: 0.4097 - val_accuracy: 0.9375\n",
      "Epoch 35/100\n",
      "2016/2016 [==============================] - 1s 593us/step - loss: 0.3751 - accuracy: 0.9524 - val_loss: 0.4167 - val_accuracy: 0.9286\n",
      "Epoch 36/100\n",
      "2016/2016 [==============================] - 1s 578us/step - loss: 0.3631 - accuracy: 0.9588 - val_loss: 0.3983 - val_accuracy: 0.9420\n",
      "Epoch 37/100\n",
      "2016/2016 [==============================] - 1s 706us/step - loss: 0.3637 - accuracy: 0.9573 - val_loss: 0.4022 - val_accuracy: 0.9464\n",
      "Epoch 38/100\n",
      "2016/2016 [==============================] - 1s 575us/step - loss: 0.3589 - accuracy: 0.9588 - val_loss: 0.3783 - val_accuracy: 0.9464\n",
      "Epoch 39/100\n",
      "2016/2016 [==============================] - 1s 565us/step - loss: 0.3568 - accuracy: 0.9608 - val_loss: 0.3937 - val_accuracy: 0.9509\n",
      "Epoch 40/100\n",
      "2016/2016 [==============================] - 1s 572us/step - loss: 0.3512 - accuracy: 0.9598 - val_loss: 0.4149 - val_accuracy: 0.9375\n",
      "Epoch 41/100\n",
      "2016/2016 [==============================] - 1s 592us/step - loss: 0.3526 - accuracy: 0.9573 - val_loss: 0.3913 - val_accuracy: 0.9375\n",
      "Epoch 42/100\n",
      "2016/2016 [==============================] - 1s 564us/step - loss: 0.3503 - accuracy: 0.9519 - val_loss: 0.3988 - val_accuracy: 0.9330\n",
      "Epoch 43/100\n",
      "2016/2016 [==============================] - 1s 561us/step - loss: 0.3487 - accuracy: 0.9539 - val_loss: 0.3760 - val_accuracy: 0.9554\n",
      "Epoch 44/100\n",
      "2016/2016 [==============================] - 1s 566us/step - loss: 0.3447 - accuracy: 0.9593 - val_loss: 0.3869 - val_accuracy: 0.9420\n",
      "Epoch 45/100\n",
      "2016/2016 [==============================] - 1s 559us/step - loss: 0.3416 - accuracy: 0.9623 - val_loss: 0.3779 - val_accuracy: 0.9464\n",
      "Epoch 46/100\n",
      "2016/2016 [==============================] - 1s 558us/step - loss: 0.3340 - accuracy: 0.9633 - val_loss: 0.4404 - val_accuracy: 0.9107\n",
      "Epoch 47/100\n",
      "2016/2016 [==============================] - 1s 592us/step - loss: 0.3339 - accuracy: 0.9633 - val_loss: 0.3726 - val_accuracy: 0.9464\n",
      "Epoch 48/100\n",
      "2016/2016 [==============================] - 1s 556us/step - loss: 0.3358 - accuracy: 0.9653 - val_loss: 0.3727 - val_accuracy: 0.9554\n",
      "Epoch 49/100\n",
      "2016/2016 [==============================] - 1s 556us/step - loss: 0.3290 - accuracy: 0.9673 - val_loss: 0.3784 - val_accuracy: 0.9330\n",
      "Epoch 50/100\n",
      "2016/2016 [==============================] - 1s 563us/step - loss: 0.3263 - accuracy: 0.9688 - val_loss: 0.4929 - val_accuracy: 0.9018\n",
      "Epoch 51/100\n",
      "2016/2016 [==============================] - 1s 560us/step - loss: 0.3266 - accuracy: 0.9673 - val_loss: 0.4172 - val_accuracy: 0.9330\n",
      "Epoch 52/100\n",
      "2016/2016 [==============================] - 1s 556us/step - loss: 0.3244 - accuracy: 0.9653 - val_loss: 0.3730 - val_accuracy: 0.9554\n",
      "Epoch 53/100\n",
      "2016/2016 [==============================] - 1s 554us/step - loss: 0.3236 - accuracy: 0.9628 - val_loss: 0.3630 - val_accuracy: 0.9464\n",
      "Epoch 54/100\n",
      "2016/2016 [==============================] - 1s 600us/step - loss: 0.3204 - accuracy: 0.9707 - val_loss: 0.4087 - val_accuracy: 0.9375\n",
      "Epoch 55/100\n",
      "2016/2016 [==============================] - 1s 581us/step - loss: 0.3237 - accuracy: 0.9648 - val_loss: 0.4200 - val_accuracy: 0.9107\n",
      "Epoch 56/100\n",
      "2016/2016 [==============================] - 1s 559us/step - loss: 0.3184 - accuracy: 0.9648 - val_loss: 0.3447 - val_accuracy: 0.9643\n",
      "Epoch 57/100\n",
      "2016/2016 [==============================] - 1s 561us/step - loss: 0.3133 - accuracy: 0.9717 - val_loss: 0.3785 - val_accuracy: 0.9509\n",
      "Epoch 58/100\n",
      "2016/2016 [==============================] - 1s 559us/step - loss: 0.3167 - accuracy: 0.9683 - val_loss: 0.3612 - val_accuracy: 0.9464\n",
      "Epoch 59/100\n",
      "2016/2016 [==============================] - 1s 556us/step - loss: 0.3111 - accuracy: 0.9692 - val_loss: 0.3533 - val_accuracy: 0.9598\n",
      "Epoch 60/100\n",
      "2016/2016 [==============================] - 1s 562us/step - loss: 0.3135 - accuracy: 0.9668 - val_loss: 0.3404 - val_accuracy: 0.9643\n",
      "Epoch 61/100\n",
      "2016/2016 [==============================] - 1s 584us/step - loss: 0.3092 - accuracy: 0.9683 - val_loss: 0.3513 - val_accuracy: 0.9509\n",
      "Epoch 62/100\n",
      "2016/2016 [==============================] - 1s 564us/step - loss: 0.3047 - accuracy: 0.9673 - val_loss: 0.3453 - val_accuracy: 0.9598\n",
      "Epoch 63/100\n",
      "2016/2016 [==============================] - 1s 560us/step - loss: 0.3073 - accuracy: 0.9668 - val_loss: 0.3497 - val_accuracy: 0.9464\n",
      "Epoch 64/100\n",
      "2016/2016 [==============================] - 1s 559us/step - loss: 0.3012 - accuracy: 0.9707 - val_loss: 0.3417 - val_accuracy: 0.9554\n",
      "Epoch 65/100\n",
      "2016/2016 [==============================] - 1s 558us/step - loss: 0.3045 - accuracy: 0.9692 - val_loss: 0.3471 - val_accuracy: 0.9509\n",
      "Epoch 66/100\n",
      "2016/2016 [==============================] - 1s 561us/step - loss: 0.3027 - accuracy: 0.9702 - val_loss: 0.3489 - val_accuracy: 0.9554\n",
      "Epoch 67/100\n",
      "2016/2016 [==============================] - 1s 584us/step - loss: 0.3017 - accuracy: 0.9697 - val_loss: 0.3375 - val_accuracy: 0.9598\n",
      "Epoch 68/100\n",
      "2016/2016 [==============================] - 1s 558us/step - loss: 0.2997 - accuracy: 0.9717 - val_loss: 0.3766 - val_accuracy: 0.9509\n",
      "Epoch 69/100\n",
      "2016/2016 [==============================] - 1s 566us/step - loss: 0.3001 - accuracy: 0.9727 - val_loss: 0.3451 - val_accuracy: 0.9554\n",
      "Epoch 70/100\n",
      "2016/2016 [==============================] - 1s 563us/step - loss: 0.2979 - accuracy: 0.9678 - val_loss: 0.3468 - val_accuracy: 0.9554\n",
      "Epoch 71/100\n",
      "2016/2016 [==============================] - 1s 560us/step - loss: 0.2930 - accuracy: 0.9732 - val_loss: 0.3558 - val_accuracy: 0.9420\n",
      "Epoch 72/100\n",
      "2016/2016 [==============================] - 1s 562us/step - loss: 0.2914 - accuracy: 0.9727 - val_loss: 0.3312 - val_accuracy: 0.9554\n",
      "Epoch 73/100\n",
      "2016/2016 [==============================] - 1s 555us/step - loss: 0.2973 - accuracy: 0.9697 - val_loss: 0.3265 - val_accuracy: 0.9598\n",
      "Epoch 74/100\n",
      "2016/2016 [==============================] - 1s 583us/step - loss: 0.2940 - accuracy: 0.9678 - val_loss: 0.3355 - val_accuracy: 0.9554\n",
      "Epoch 75/100\n",
      "2016/2016 [==============================] - 1s 560us/step - loss: 0.2908 - accuracy: 0.9717 - val_loss: 0.3512 - val_accuracy: 0.9420\n",
      "Epoch 76/100\n",
      "2016/2016 [==============================] - 1s 556us/step - loss: 0.2915 - accuracy: 0.9737 - val_loss: 0.3320 - val_accuracy: 0.9554\n",
      "Epoch 77/100\n",
      "2016/2016 [==============================] - 1s 565us/step - loss: 0.2871 - accuracy: 0.9752 - val_loss: 0.3218 - val_accuracy: 0.9643\n",
      "Epoch 78/100\n",
      "2016/2016 [==============================] - 1s 579us/step - loss: 0.2861 - accuracy: 0.9732 - val_loss: 0.3224 - val_accuracy: 0.9598\n",
      "Epoch 79/100\n",
      "2016/2016 [==============================] - 1s 562us/step - loss: 0.2867 - accuracy: 0.9692 - val_loss: 0.3379 - val_accuracy: 0.9509\n",
      "Epoch 80/100\n",
      "2016/2016 [==============================] - 1s 588us/step - loss: 0.2849 - accuracy: 0.9762 - val_loss: 0.3421 - val_accuracy: 0.9643\n",
      "Epoch 81/100\n",
      "2016/2016 [==============================] - 1s 580us/step - loss: 0.2778 - accuracy: 0.9767 - val_loss: 0.3777 - val_accuracy: 0.9375\n",
      "Epoch 82/100\n",
      "2016/2016 [==============================] - 1s 556us/step - loss: 0.2842 - accuracy: 0.9732 - val_loss: 0.3888 - val_accuracy: 0.9420\n",
      "Epoch 83/100\n",
      "2016/2016 [==============================] - 1s 557us/step - loss: 0.2818 - accuracy: 0.9722 - val_loss: 0.3313 - val_accuracy: 0.9598\n",
      "Epoch 84/100\n",
      "2016/2016 [==============================] - 1s 561us/step - loss: 0.2801 - accuracy: 0.9742 - val_loss: 0.3260 - val_accuracy: 0.9598\n",
      "Epoch 85/100\n",
      "2016/2016 [==============================] - 1s 588us/step - loss: 0.2780 - accuracy: 0.9752 - val_loss: 0.3347 - val_accuracy: 0.9554\n",
      "Epoch 86/100\n",
      "2016/2016 [==============================] - 1s 556us/step - loss: 0.2751 - accuracy: 0.9752 - val_loss: 0.3305 - val_accuracy: 0.9554\n",
      "Epoch 87/100\n",
      "2016/2016 [==============================] - 1s 561us/step - loss: 0.2745 - accuracy: 0.9802 - val_loss: 0.3226 - val_accuracy: 0.9643\n",
      "Epoch 88/100\n",
      "2016/2016 [==============================] - 1s 555us/step - loss: 0.2745 - accuracy: 0.9707 - val_loss: 0.3247 - val_accuracy: 0.9688\n",
      "Epoch 89/100\n",
      "2016/2016 [==============================] - 1s 555us/step - loss: 0.2754 - accuracy: 0.9767 - val_loss: 0.3159 - val_accuracy: 0.9643\n",
      "Epoch 90/100\n",
      "2016/2016 [==============================] - 1s 561us/step - loss: 0.2743 - accuracy: 0.9747 - val_loss: 0.3254 - val_accuracy: 0.9464\n",
      "Epoch 91/100\n",
      "2016/2016 [==============================] - 1s 590us/step - loss: 0.2763 - accuracy: 0.9742 - val_loss: 0.3757 - val_accuracy: 0.9464\n",
      "Epoch 92/100\n",
      "2016/2016 [==============================] - 1s 553us/step - loss: 0.2715 - accuracy: 0.9737 - val_loss: 0.3438 - val_accuracy: 0.9598\n",
      "Epoch 93/100\n",
      "2016/2016 [==============================] - 1s 555us/step - loss: 0.2693 - accuracy: 0.9722 - val_loss: 0.3151 - val_accuracy: 0.9688\n",
      "Epoch 94/100\n",
      "2016/2016 [==============================] - 1s 562us/step - loss: 0.2751 - accuracy: 0.9732 - val_loss: 0.3112 - val_accuracy: 0.9643\n",
      "Epoch 95/100\n",
      "2016/2016 [==============================] - 1s 553us/step - loss: 0.2644 - accuracy: 0.9777 - val_loss: 0.3316 - val_accuracy: 0.9375\n",
      "Epoch 96/100\n",
      "2016/2016 [==============================] - 1s 556us/step - loss: 0.2670 - accuracy: 0.9747 - val_loss: 0.3135 - val_accuracy: 0.9643\n",
      "Epoch 97/100\n",
      "2016/2016 [==============================] - 1s 582us/step - loss: 0.2664 - accuracy: 0.9742 - val_loss: 0.3119 - val_accuracy: 0.9598\n",
      "Epoch 98/100\n",
      "2016/2016 [==============================] - 1s 556us/step - loss: 0.2660 - accuracy: 0.9757 - val_loss: 0.3431 - val_accuracy: 0.9554\n",
      "Epoch 99/100\n",
      "2016/2016 [==============================] - 1s 554us/step - loss: 0.2620 - accuracy: 0.9777 - val_loss: 0.3075 - val_accuracy: 0.9643\n",
      "Epoch 100/100\n",
      "2016/2016 [==============================] - 1s 564us/step - loss: 0.2656 - accuracy: 0.9747 - val_loss: 0.3221 - val_accuracy: 0.9554\n",
      "18/18 [==============================] - 0s 607us/step - loss: 0.2858 - accuracy: 0.9643\n",
      "Test Accuracy 96.43%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32,input_shape=(X_train.shape[1],),activation='relu',kernel_regularizer=l2(0.01)))\n",
    "model.add(Dense(16,activation='relu',kernel_regularizer=l2(0.01)))\n",
    "model.add(Dense(8,activation='relu',kernel_regularizer=l2(0.01)))\n",
    "model.add(Dense(Y.shape[1],activation='softmax'))\n",
    "\n",
    "optimier = Adam(learning_rate=0.0001)\n",
    "\n",
    "model.compile(optimizer=optimier,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train,y_train,epochs=100,batch_size=1,validation_split=0.1)\n",
    "\n",
    "loss ,accuracy = model.evaluate(X_test,y_test)\n",
    "print(f'Test Accuracy {accuracy *100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n",
      "class:['sad']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "feature_columns = [f'{i}' for i in range(166)]\n",
    "X_new = test_data[feature_columns].values\n",
    "\n",
    "model = load_model('model.h5')\n",
    "\n",
    "predictions = model.predict(X_new)\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "predicted_labels = predictions.argmax(axis=1)\n",
    "emotion_labels = ['angry', 'sad', 'disgust', 'happy', 'pleasant_surprised', 'neutral', 'fear']\n",
    "\n",
    "predicted_class_names = [emotion_labels[i] for i in predicted_labels]\n",
    "\n",
    "print(f'class:{predicted_class_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
